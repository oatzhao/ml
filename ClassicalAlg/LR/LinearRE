线性回归：

y = m1x1 + m2x2 + b

实质：

一元线性回归其实就是去找到一条直线，这条直线能以最小的误差（Loss）来拟合数据

去找每个点和直线距离|y - ( mx + b )|最小的那条线

Loss = sum(sqr( yi - ( mxi + b)))/n

最小化误差：

（1）最小二乘法

其中x、y、i、n都是已知，可以将loss看作m、b的二次方程。那么求loss最小值的问题转变成求极值的问题

复合函数求导法法d(f(g(x))) = df(Y)*dg(x)

dLoss/dm = -2sum(xi( yi - ( mxi + b)))/n = 0

dLoss/db = -2sum( yi - ( mxi + b))/n = 0

（2）梯度下降法

b = b + dLoss/db

m = m + dLoss/dm

备注：

线性回归适用于线性变换数据

线性回归受噪音影响较大

求解方法有两种，一种批量梯度下降

一种正规方程方法

前者是一种通过迭代求得的数值解

后者是一种通过的公式一步到位求得的解析解，适用于特征个数较少的情况

另外，先对特征标准化可以加快求解速度

批量梯度下降法：θj := θj − α· ∂J(θ)/∂θj  (j = 0,1,...,n, α为学习速率, J(θ)/∂θj 为J的偏导数)  不断同时更新θj直到收敛

正规方程法：θ = (XTX)−1XTY (T是转置的意思)

布局加权回归（LWR）：

详情参见吴恩达机器学习第三节课

局部加权回归的思想是重点考虑输入特征X附近的情况，随着距离特征X越远，权重越小

与线性回归的损失函数相比，多了一个W权值，其中x是要预测的特征

T是带宽参数，用来调节"局部"的大小

求出参数θ的方法有以下两种：

批量梯度下降法：θj := θj − α· ∂J(θ)/∂θj  (j = 0,1,...,n, α为学习速率, J(θ)/∂θj 为J的偏导数)  不断同时更新θj直到收敛

正规方程法：θ = (XTWX)−1XTWY (T是转置的意思)










￼