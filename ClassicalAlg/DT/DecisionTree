决策树：

一种基于分类和回归的方法

分类决策树模型是一种描述对实例进行分类的树形结构

组成：

（1）结点 （2）有向边

结点类型：

（1）内部结点 表示一个特征或属性

（2）叶结点 表示一个类

决策时的路径或其对应的if-then规则集合具有一个重要的性质：互斥并且完备

每一个实例都被一条路径或一条规则所覆盖，而且只被一条规则所覆盖

处理过程：

（1）收集数据

（2）准备数据

（3）分析数据

（4）训练算法

（5）测试算法

（6）使用算法

构建决策树：

（1）特征选择

（2）决策树生成

（3）决策树的修剪

香农熵:

熵是物体在一个一定的宏观状态下所有微观状态的总和。这是目前物理上对熵理解的最透彻的定义。

熵(entropy):上面的Ⅰ(x)是指在某个概率分布之下,某个概率值对应的信息量的公式.那么我们要知道这整个概率分布对应的信息量的平均值.这个平均值就叫做随机变量x的熵

特征X取某个固定值时的信息含量是-lnP（X(i)），例如太阳从西边升起

太阳从哪个方向生升起的熵是从西边升起、从东边升起各种可能的概率分布的平均值

例如太阳从东边升起概率3/4，从西边升起1/4，从南边升起1/4，从北边升起1/4

特征X的熵：H(x) = -3/4 * ln3/4 - 1/4 * ln1/4 ……

条件熵：

条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵(conditional entropy) H(Y|X)，

定义X给定条件下Y的条件概率分布的熵对X的数学期望

信息增益：

得知特征X的信息而是的类Y的信息的不确定性减少的程度。特征A对训练数据集D的信息增益为g(D, A) = H(D) - H(D|A)

信息增益比：

gR(D,A)=g(D,A)/HA(D)

HA(D)=−∑ni=1|Di||D|log2|Di||D|

决策树的生成：

（1）ID3

每次选择信息增益最大的特种作为判断模块建立子结点

ID3算法可用于划分标准型数据集，没有减枝的过程，为了解决过度数据匹配的问题，可以通过裁剪合并相邻的无法产生大量信息增益的叶子结点

使用信息增益的一个缺点是，它偏向于具有大量值的属性。也就是说训练集中，某个属性所取的不同值的个数越多，那么越有可能拿来作为分裂属性

例如id号是无意义的

ID3不能处理连续分布的数据特征

算法步骤：

输入：训练数据集D,特征集A，阈值ε

输出：决策树T

（1）若D中所有实例属于同一类Ck，则T为单结点树，并将类Ck作为该结点的类标记，返回T

（2）若A = Null，则T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T;

（3）否则计算A中各特征对D的信息增益，选择信息增益最大的特征Ag

（4）如果Ag的信息增益小于阈值ε，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T

（5）否则，对Ag的每一种可能ai，依Ag = ai将D分割为若干非空子集Dj，将Dj中实例数最大的类作为标记，构建子结点，

由子结点及其子结点构成树T，返回T

（6）对第i各子结点，以Di为训练集，以A-{Ag}为特征集，递归地调用步骤（1）～（5），得到子树Ti,返回Ti























https://blog.csdn.net/xierhacker/article/details/53463567
















































