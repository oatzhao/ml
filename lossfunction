（1）0-1损失函数：

L(Y,f(X))={1,Y≠f(X)
           0,Y=f(X)


预测错误时，损失函数值为1，预测正确时，损失函数值为0

该损失函数不考虑预测值和真实值的误差程度，也就是只要预测错误，预测错误差一点和差很多是一样的

（2）平方损失函数：

L(Y,f(X))=(Y−f(X))2

取预测差距的平方

（3）绝对值损失函数：

L(Y,f(X))=|Y−f(X)|

该损失函数的意义和平方损失函数差不多，只不过是取了绝对值而不是求绝对值，差距不会被平方放大

（4）对数损失函数：

L(Y,P(Y|X))=−logP(Y|X)

该损失函数用到了极大似然估计的思想。P(Y|X)通俗的解释就是：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。

由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，

其损失值应该是越小，因此再加个负号取个反。

（5）全局损失函数

J(w,b)=1/2m ∑(Y−f(X))2

最大似然估计：

已经拿到了很多样本，这些样本值已经实现，最大似然估计就是去找哪个参数估计值，使得前面已经实现的样本值发生概率最大

因为手头上的样本已经实现，发生概率最大才复合逻辑。这时是求样本所有观测的联合概率最大化，是个连乘积，只要取对数，就变成了线性加和。

此时通过参数求导，并令一阶导数为零，就可以通过方程组，得到最大似然估计值

最小二乘：

https://www.zhihu.com/question/20447622/answer/173149479（对矩阵求解比较易于理解的解释）

找到一组估计值，使得实际值与估计值之间的距离最小。本来用两者差的绝对值汇总并使之最小是最理想的，但绝对值在数学上求最小值比较麻烦，

因此替代做法是，找一个估计值，使实际值与估计值之差的平方和总之后的值最小






































